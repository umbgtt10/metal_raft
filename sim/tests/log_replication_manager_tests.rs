// Copyright 2025 Umberto Gotti <umberto.gotti@umbertogotti.dev>
// Licensed under the Apache License, Version 2.0
// http://www.apache.org/licenses/LICENSE-2.0

use raft_core::{
    collections::{
        configuration::Configuration, log_entry_collection::LogEntryCollection,
        map_collection::MapCollection, node_collection::NodeCollection,
    },
    components::log_replication_manager::LogReplicationManager,
    log_entry::{EntryType, LogEntry},
    node_state::NodeState,
    raft_messages::RaftMsg,
    snapshot::{Snapshot, SnapshotMetadata},
    state_machine::StateMachine,
    storage::Storage,
};
use raft_sim::{
    in_memory_chunk_collection::InMemoryChunkCollection,
    in_memory_config_change_collection::InMemoryConfigChangeCollection,
    in_memory_log_entry_collection::InMemoryLogEntryCollection,
    in_memory_map_collection::InMemoryMapCollection,
    in_memory_node_collection::InMemoryNodeCollection,
    in_memory_state_machine::InMemoryStateMachine, in_memory_storage::InMemoryStorage,
};

// ============================================================
// handle_append_entries tests (Follower receiving)
// ============================================================

#[test]
fn test_liveness_accept_entries_from_leader() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);
    let mut current_term = 2;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    let entries = InMemoryLogEntryCollection::new(&[
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd1".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd2".to_string()),
        },
    ]);

    let (response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        2, // term
        0, // prev_log_index
        0, // prev_log_term
        entries,
        2, // leader_commit
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    match response {
        RaftMsg::AppendEntriesResponse { term, success, .. } => {
            assert_eq!(term, 2);
            assert!(success, "Should accept entries from current leader");
        }
        _ => panic!("Expected AppendEntriesResponse"),
    }

    assert_eq!(storage.last_log_index(), 2);
    assert_eq!(replication.commit_index(), 2);
}

#[test]
fn test_safety_reject_entries_from_stale_term() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(5);
    let mut current_term = 5;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    let entries = InMemoryLogEntryCollection::new(&[LogEntry {
        term: 3,
        entry_type: EntryType::Command("cmd1".to_string()),
    }]);

    let (response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        3, // stale term
        0, // prev_log_index
        0, // prev_log_term
        entries,
        1, // leader_commit
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    match response {
        RaftMsg::AppendEntriesResponse { term, success, .. } => {
            assert_eq!(term, 5);
            assert!(!success, "Should reject entries from stale term");
        }
        _ => panic!("Expected AppendEntriesResponse"),
    }

    assert_eq!(storage.last_log_index(), 0, "Should not append any entries");
}

#[test]
fn test_safety_reject_inconsistent_prev_log() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);
    let mut current_term = 2;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Storage has no entries, but leader assumes we have entry at index 5
    let entries = InMemoryLogEntryCollection::new(&[LogEntry {
        term: 2,
        entry_type: EntryType::Command("cmd".to_string()),
    }]);

    let (response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        2,
        5, // prev_log_index (we don't have this!)
        2, // prev_log_term
        entries,
        1,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    match response {
        RaftMsg::AppendEntriesResponse { success, .. } => {
            assert!(!success, "Should reject due to missing prev_log entry");
        }
        _ => panic!("Expected AppendEntriesResponse"),
    }
}

#[test]
fn test_safety_reject_mismatched_prev_log_term() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3);

    // Add an entry at index 1 with term 1
    storage.append_entries(&[LogEntry {
        term: 1,
        entry_type: EntryType::Command("old".to_string()),
    }]);

    let mut current_term = 3;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    let entries = InMemoryLogEntryCollection::new(&[LogEntry {
        term: 3,
        entry_type: EntryType::Command("new".to_string()),
    }]);

    // Leader thinks our entry at index 1 has term 2 (but we have term 1)
    let (response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        3,
        1, // prev_log_index
        2, // prev_log_term (mismatch!)
        entries,
        2,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    match response {
        RaftMsg::AppendEntriesResponse { success, .. } => {
            assert!(!success, "Should reject due to prev_log_term mismatch");
        }
        _ => panic!("Expected AppendEntriesResponse"),
    }
}

#[test]
fn test_safety_delete_conflicting_entries() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);

    // Initial log: [term1:cmd1, term1:cmd2, term1:cmd3]
    storage.append_entries(&[
        LogEntry {
            term: 1,
            entry_type: EntryType::Command("cmd1".to_string()),
        },
        LogEntry {
            term: 1,
            entry_type: EntryType::Command("cmd2".to_string()),
        },
        LogEntry {
            term: 1,
            entry_type: EntryType::Command("cmd3".to_string()),
        },
    ]);

    let mut current_term = 2;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Leader sends entry at index 2 with different term (conflict!)
    let entries = InMemoryLogEntryCollection::new(&[LogEntry {
        term: 2,
        entry_type: EntryType::Command("new_cmd2".to_string()),
    }]);

    let (_response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        2,
        1, // prev_log_index (entry 1 matches)
        1, // prev_log_term
        entries,
        2,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    // Should have deleted cmd3 and replaced cmd2
    assert_eq!(storage.last_log_index(), 2);
    let log_entries = storage.get_entries(1, 3);
    assert_eq!(log_entries.len(), 2);
    // First entry unchanged
    assert_eq!(log_entries.as_slice()[0].term, 1);
    // Second entry replaced
    assert_eq!(log_entries.as_slice()[1].term, 2);
}

#[test]
fn test_liveness_heartbeat_updates_commit_index() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);

    // Add some entries
    storage.append_entries(&[
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd1".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd2".to_string()),
        },
    ]);

    let mut current_term = 2;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Empty AppendEntries (heartbeat) with leader_commit = 2
    let entries = InMemoryLogEntryCollection::new(&[]);

    let (_response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        2,
        2, // prev_log_index
        2, // prev_log_term
        entries,
        2, // leader_commit
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    assert_eq!(
        replication.commit_index(),
        2,
        "Should update commit index from heartbeat"
    );
}

#[test]
fn test_safety_step_down_on_higher_term_append_entries() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);
    let mut current_term = 2;
    let mut role = NodeState::Leader;
    let mut state_machine = InMemoryStateMachine::new();

    let entries = InMemoryLogEntryCollection::new(&[]);

    let (_response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        5, // higher term
        0,
        0,
        entries,
        0,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    assert_eq!(current_term, 5, "Should update to higher term");
    assert_eq!(role, NodeState::Follower, "Should step down to follower");
}

// ============================================================
// handle_append_entries_response tests (Leader processing)
// ============================================================

#[test]
fn test_liveness_update_next_index_on_success() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);

    // Add 5 entries
    storage.append_entries(&[
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd1".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd2".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd3".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd4".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd5".to_string()),
        },
    ]);

    let mut peers = InMemoryNodeCollection::new();
    peers.push(2).unwrap();
    peers.push(3).unwrap();
    let config = Configuration::new(peers.clone());

    replication.initialize_leader_state(peers.iter(), &storage);

    let mut state_machine = InMemoryStateMachine::new();

    // Node 2 successfully replicated up to index 3
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(
            2,    // from
            true, // success
            3,    // match_index
            &storage,
            &mut state_machine,
            &config,
        );

    assert_eq!(
        replication.next_index().get(2),
        Some(4),
        "Should update next_index"
    );
    assert_eq!(
        replication.match_index().get(2),
        Some(3),
        "Should update match_index"
    );
}

#[test]
fn test_liveness_decrement_next_index_on_failure() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);

    storage.append_entries(&[LogEntry {
        term: 2,
        entry_type: EntryType::Command("cmd".to_string()),
    }]);

    let mut peers = InMemoryNodeCollection::new();
    peers.push(2).unwrap();
    let config = Configuration::new(peers.clone());

    replication.initialize_leader_state(peers.iter(), &storage);

    let mut state_machine = InMemoryStateMachine::new();

    // Node 2 rejects (log inconsistency)
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(
            2,
            false, // failure
            0,
            &storage,
            &mut state_machine,
            &config,
        );

    assert_eq!(
        replication.next_index().get(2),
        Some(1),
        "Should decrement next_index on failure"
    );
}

#[test]
fn test_liveness_commit_index_advancement_on_majority() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);

    // Add 5 entries
    storage.append_entries(&[
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd1".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd2".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd3".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd4".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd5".to_string()),
        },
    ]);

    let mut peers = InMemoryNodeCollection::new();
    peers.push(2).unwrap();
    peers.push(3).unwrap();
    peers.push(4).unwrap();
    peers.push(5).unwrap();
    let config = Configuration::new(peers.clone());

    replication.initialize_leader_state(peers.iter(), &storage);

    let mut state_machine = InMemoryStateMachine::new();

    // Leader (self) has all 5 entries
    // Node 2 confirms up to index 3
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(2, true, 3, &storage, &mut state_machine, &config);

    // Node 3 confirms up to index 3
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(3, true, 3, &storage, &mut state_machine, &config);

    // Now 3 out of 5 nodes (including leader) have entries up to index 3
    // Should advance commit_index to 3
    assert_eq!(
        replication.commit_index(),
        3,
        "Should advance commit_index on majority"
    );

    // Node 4 confirms up to index 5
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(4, true, 5, &storage, &mut state_machine, &config);

    // Now 3 out of 5 have up to index 5 (leader, node 4, and we need one more)
    // commit_index should still be 3
    assert_eq!(
        replication.commit_index(),
        3,
        "Should not advance without majority at higher index"
    );

    // Node 5 confirms up to index 4
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(5, true, 4, &storage, &mut state_machine, &config);

    // Now we have: leader(5), node2(3), node3(3), node4(5), node5(4)
    // Majority (3/5) at index 4: leader, node4, node5
    assert_eq!(
        replication.commit_index(),
        4,
        "Should advance to index 4 with majority"
    );
}

#[test]
fn test_safety_only_commit_current_term_entries() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3);

    // Add entries from previous terms and current term
    storage.append_entries(&[
        LogEntry {
            term: 1,
            entry_type: EntryType::Command("old1".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("old2".to_string()),
        },
        LogEntry {
            term: 3,
            entry_type: EntryType::Command("new1".to_string()),
        },
    ]);

    let mut peers = InMemoryNodeCollection::new();
    peers.push(2).unwrap();
    peers.push(3).unwrap();
    let config = Configuration::new(peers.clone());

    replication.initialize_leader_state(peers.iter(), &storage);

    let mut state_machine = InMemoryStateMachine::new();

    // Node 2 and 3 both have up to index 2 (old term entries)
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(2, true, 2, &storage, &mut state_machine, &config);
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(3, true, 2, &storage, &mut state_machine, &config);

    // Even though majority has index 2, we shouldn't commit old term entries directly
    // (Raft safety: only commit entries from current term via replication)
    // But actually, when we replicate index 3 and it gets majority, indexes 1 and 2 get committed too

    // Let's verify by replicating index 3
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(2, true, 3, &storage, &mut state_machine, &config);

    // Now majority has index 3 (current term), so commit_index should be 3
    assert_eq!(replication.commit_index(), 3);
}

#[test]
fn test_safety_ignore_responses_from_old_term() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3);

    storage.append_entries(&[LogEntry {
        term: 3,
        entry_type: EntryType::Command("cmd".to_string()),
    }]);

    let mut peers = InMemoryNodeCollection::new();
    peers.push(2).unwrap();
    let config = Configuration::new(peers.clone());

    replication.initialize_leader_state(peers.iter(), &storage);

    let initial_next = replication.next_index().get(2);
    let initial_match = replication.match_index().get(2);

    let mut state_machine = InMemoryStateMachine::new();

    // Response from node 2 with a really high match_index (should update)
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(
            2,
            true,
            5, // Very high match index
            &storage,
            &mut state_machine,
            &config,
        );

    // next_index and match_index should be updated (note: the API doesn't check term in response)
    // This test needs revision based on actual API behavior
    // For now, we verify that the state was updated
    assert!(replication.next_index().get(2) >= initial_next);
    assert!(replication.match_index().get(2) >= initial_match);
}

// ============================================================
// Edge cases and initialization
// ============================================================

#[test]
fn test_liveness_initialize_leader_state() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(1);

    // Add 10 entries
    for i in 1..=10 {
        storage.append_entries(&[LogEntry {
            term: 1,
            entry_type: EntryType::Command(format!("cmd{}", i)),
        }]);
    }

    let mut peers = InMemoryNodeCollection::new();
    peers.push(2).unwrap();
    peers.push(3).unwrap();
    peers.push(4).unwrap();

    replication.initialize_leader_state(peers.iter(), &storage);

    // All peers should have next_index = log_length + 1 = 11
    assert_eq!(replication.next_index().get(2), Some(11));
    assert_eq!(replication.next_index().get(3), Some(11));
    assert_eq!(replication.next_index().get(4), Some(11));

    // All peers should have match_index = 0
    assert_eq!(replication.match_index().get(2), Some(0));
    assert_eq!(replication.match_index().get(3), Some(0));
    assert_eq!(replication.match_index().get(4), Some(0));
}

#[test]
fn test_safety_append_entries_with_exact_match() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);

    // Existing log: [term1:cmd1, term2:cmd2]
    storage.append_entries(&[
        LogEntry {
            term: 1,
            entry_type: EntryType::Command("cmd1".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd2".to_string()),
        },
    ]);

    let mut current_term = 2;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Leader sends same entries (idempotent append)
    let entries = InMemoryLogEntryCollection::new(&[
        LogEntry {
            term: 1,
            entry_type: EntryType::Command("cmd1".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd2".to_string()),
        },
    ]);

    let (_response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        2,
        0,
        0,
        entries,
        2,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    // Should still have 2 entries (no duplicates)
    assert_eq!(storage.last_log_index(), 2);
}

#[test]
fn test_safety_commit_index_never_decreases() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);

    storage.append_entries(&[
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd1".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd2".to_string()),
        },
    ]);

    let mut current_term = 2;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // First set commit index to 2
    let entries = InMemoryLogEntryCollection::new(&[]);

    let (_response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        2,
        2,
        2,
        entries,
        2, // leader_commit = 2
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    assert_eq!(replication.commit_index(), 2);

    // Leader sends heartbeat with lower commit index
    let entries2 = InMemoryLogEntryCollection::new(&[]);
    let (_response2, _config_changes2) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        2,
        2,
        2,
        entries2,
        1, // leader_commit = 1 (lower than our 2)
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    // commit_index should not decrease
    assert_eq!(
        replication.commit_index(),
        2,
        "commit_index should never decrease"
    );
}

#[test]
fn test_liveness_three_node_cluster_majority() {
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(1);

    storage.append_entries(&[
        LogEntry {
            term: 1,
            entry_type: EntryType::Command("cmd1".to_string()),
        },
        LogEntry {
            term: 1,
            entry_type: EntryType::Command("cmd2".to_string()),
        },
    ]);

    let mut peers = InMemoryNodeCollection::new();
    peers.push(2).unwrap();
    peers.push(3).unwrap();
    let config = Configuration::new(peers.clone());

    replication.initialize_leader_state(peers.iter(), &storage);

    let mut state_machine = InMemoryStateMachine::new();

    // Only node 2 confirms (1 follower + 1 leader = 2/3 = majority)
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(2, true, 2, &storage, &mut state_machine, &config);

    assert_eq!(
        replication.commit_index(),
        2,
        "Should commit with 2/3 majority"
    );
}

// ============================================================
// Log Compaction / Snapshot Tests
// ============================================================

#[test]
fn test_liveness_get_append_entries_with_compacted_snapshot_point() {
    // Test: Leader sends AppendEntries where prev_log_index matches snapshot point
    // Expected: Should use snapshot metadata for prev_log_term
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3);

    // Create log with entries 1-15
    let entries: Vec<LogEntry<String>> = (1..=15)
        .map(|i| LogEntry {
            term: 2,
            entry_type: EntryType::Command(format!("cmd{}", i)),
        })
        .collect();
    storage.append_entries(&entries);

    // Create snapshot at index 10, term 2
    let mut state_machine = InMemoryStateMachine::new();
    for i in 1..=10 {
        state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = state_machine.create_snapshot();

    let snapshot = Snapshot {
        metadata: SnapshotMetadata {
            last_included_index: 10,
            last_included_term: 2,
        },
        data: snapshot_data,
    };
    storage.save_snapshot(snapshot);

    // Compact the log - discard entries 1-10
    storage.discard_entries_before(11);

    // Now first_log_index = 11, entries 11-15 remain in log
    assert_eq!(storage.first_log_index(), 11);
    assert_eq!(storage.last_log_index(), 15);
    assert!(
        storage.get_entry(10).is_none(),
        "Entry 10 should be compacted"
    );

    // Initialize leader state with one peer
    let mut peers = InMemoryNodeCollection::new();
    peers.push(2).unwrap();
    let config = Configuration::new(peers.clone());

    replication.initialize_leader_state(peers.iter(), &storage);

    // Simulate that follower 2 confirmed up to index 10
    // This will set next_index[2] = 11 (one after the confirmed match)
    let _config_changes: InMemoryConfigChangeCollection = replication
        .handle_append_entries_response(2, true, 10, &storage, &mut state_machine, &config);

    // Now next_index[2] = 11, so prev_log_index will be 10
    let msg = replication.get_append_entries_for_follower::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryStorage>(2, 3, &storage);

    match msg {
        RaftMsg::AppendEntries {
            term,
            prev_log_index,
            prev_log_term,
            entries: _,
            leader_commit: _,
        } => {
            assert_eq!(term, 3);
            assert_eq!(
                prev_log_index, 10,
                "prev_log_index should be at snapshot point"
            );
            assert_eq!(
                prev_log_term, 2,
                "prev_log_term should come from snapshot metadata"
            );
        }
        _ => panic!("Expected AppendEntries message"),
    }
}

#[test]
fn test_safety_get_append_entries_before_snapshot_point() {
    // Test: Leader tries to send AppendEntries where prev_log_index is before snapshot
    // Expected: Returns prev_log_term = 0 (signals need for InstallSnapshot RPC)
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3);

    // Create log with entries 1-15
    let entries: Vec<LogEntry<String>> = (1..=15)
        .map(|i| LogEntry {
            term: 2,
            entry_type: raft_core::log_entry::EntryType::Command(format!("cmd{}", i)),
        })
        .collect();
    storage.append_entries(&entries);

    // Create snapshot at index 10
    let mut state_machine = InMemoryStateMachine::new();
    for i in 1..=10 {
        state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = state_machine.create_snapshot();

    let snapshot = Snapshot {
        metadata: SnapshotMetadata {
            last_included_index: 10,
            last_included_term: 2,
        },
        data: snapshot_data,
    };
    storage.save_snapshot(snapshot);

    // Compact the log - discard entries 1-10
    storage.discard_entries_before(11);

    // Initialize leader state with one peer
    let mut peers = InMemoryNodeCollection::new();
    peers.push(2).unwrap();
    let config = Configuration::new(peers.clone());
    replication.initialize_leader_state(peers.iter(), &storage);

    // Simulate follower rejections to decrement next_index below snapshot point
    // Each rejection decrements next_index by 1
    for _ in 0..7 {
        // Decrement from 11 down to 4
        let _config_changes: InMemoryConfigChangeCollection = replication
            .handle_append_entries_response(2, false, 0, &storage, &mut state_machine, &config);
    }

    let msg = replication.get_append_entries_for_follower::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryStorage>(2, 3, &storage);

    match msg {
        RaftMsg::AppendEntries {
            term,
            prev_log_index,
            prev_log_term,
            entries: _,
            leader_commit: _,
        } => {
            assert_eq!(term, 3);
            assert!(
                prev_log_index < 11,
                "prev_log_index should be below first_log_index"
            );
            assert_eq!(
                prev_log_term, 0,
                "prev_log_term should be 0 (entry before snapshot, needs InstallSnapshot)"
            );
        }
        _ => panic!("Expected AppendEntries message"),
    }
}

#[test]
fn test_safety_follower_rejects_append_with_compacted_prev_log() {
    // Test: Follower with compacted log receives AppendEntries with prev_log_index
    // that was compacted but doesn't match snapshot point
    // Expected: Rejects the append (consistency check fails)
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3);
    let mut current_term = 3;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Follower has entries 1-20
    let entries: Vec<LogEntry<String>> = (1..=20)
        .map(|i| LogEntry {
            term: 2,
            entry_type: raft_core::log_entry::EntryType::Command(format!("cmd{}", i)),
        })
        .collect();
    storage.append_entries(&entries);

    // Create snapshot at index 15
    for i in 1..=15 {
        state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = state_machine.create_snapshot();

    let snapshot = Snapshot {
        metadata: SnapshotMetadata {
            last_included_index: 15,
            last_included_term: 2,
        },
        data: snapshot_data,
    };
    storage.save_snapshot(snapshot);

    // Compact log - discard entries 1-15
    storage.discard_entries_before(16);

    assert_eq!(storage.first_log_index(), 16);
    assert!(
        storage.get_entry(10).is_none(),
        "Entry 10 should be compacted"
    );

    // Leader sends AppendEntries with prev_log_index=10 (compacted, not at snapshot point)
    let leader_entries = InMemoryLogEntryCollection::new(&[LogEntry {
        term: 3,
        entry_type: raft_core::log_entry::EntryType::Command("new_cmd".to_string()),
    }]);

    let (response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        3,              // term
        10,             // prev_log_index (compacted)
        2,              // prev_log_term
        leader_entries, // entries to append
        20,             // leader_commit
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    match response {
        RaftMsg::AppendEntriesResponse {
            term,
            success,
            match_index: _,
            ..
        } => {
            assert_eq!(term, 3);
            assert!(
                !success,
                "Should reject append when prev_log_index is compacted"
            );
        }
        _ => panic!("Expected AppendEntriesResponse"),
    }
}

#[test]
fn test_liveness_follower_accepts_append_at_snapshot_point() {
    // Test: Follower with snapshot receives AppendEntries where prev_log_index
    // equals snapshot's last_included_index and terms match
    // Expected: Accepts the append (though this requires check_log_consistency enhancement)
    // NOTE: This test documents current behavior - will fail until check_log_consistency
    // is enhanced to check snapshot metadata
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3);
    let mut current_term = 3;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Follower has entries 1-15
    let entries: Vec<LogEntry<String>> = (1..=15)
        .map(|i| LogEntry {
            term: 2,
            entry_type: EntryType::Command(format!("cmd{}", i)),
        })
        .collect();
    storage.append_entries(&entries);

    // Create snapshot at index 10, term 2
    for i in 1..=10 {
        state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = state_machine.create_snapshot();

    let snapshot = Snapshot {
        metadata: SnapshotMetadata {
            last_included_index: 10,
            last_included_term: 2,
        },
        data: snapshot_data,
    };
    storage.save_snapshot(snapshot);

    // Compact log - discard entries 1-10
    storage.discard_entries_before(11);

    assert_eq!(storage.first_log_index(), 11);
    assert_eq!(storage.last_log_index(), 15);

    // Leader sends AppendEntries with prev_log_index=10 (at snapshot point), prev_log_term=2
    let leader_entries = InMemoryLogEntryCollection::new(&[LogEntry {
        term: 3,
        entry_type: EntryType::Command("cmd16".to_string()),
    }]);

    let (response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        3,              // term
        10,             // prev_log_index (at snapshot point)
        2,              // prev_log_term (matches snapshot)
        leader_entries, // entries to append
        16,             // leader_commit
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    match response {
        RaftMsg::AppendEntriesResponse {
            term,
            success,
            match_index,
            ..
        } => {
            assert_eq!(term, 3);
            assert!(
                success,
                "Should succeed when prev_log_term matches snapshot term"
            );
            // After truncating to the snapshot point, the new entry becomes index 11
            assert_eq!(match_index, 11);
        }
        _ => panic!("Expected AppendEntriesResponse"),
    }

    // Verify entry was appended
    assert_eq!(storage.last_log_index(), 11);
}

#[test]
fn test_safety_follower_rejects_append_with_mismatched_snapshot_term() {
    // Test: Follower has snapshot, receives AppendEntries with prev_log_index at snapshot
    // but WRONG term - should reject
    // Expected: Rejects due to term mismatch (proves inconsistency)
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3);
    let mut current_term = 3;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Follower has entries 1-15, term 2
    let entries: Vec<LogEntry<String>> = (1..=15)
        .map(|i| LogEntry {
            term: 2,
            entry_type: EntryType::Command(format!("cmd{}", i)),
        })
        .collect();
    storage.append_entries(&entries);

    // Create snapshot at index 10, term 2
    for i in 1..=10 {
        state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = state_machine.create_snapshot();

    let snapshot = Snapshot {
        metadata: SnapshotMetadata {
            last_included_index: 10,
            last_included_term: 2, // Snapshot has term 2
        },
        data: snapshot_data,
    };
    storage.save_snapshot(snapshot);

    // Compact log - discard entries 1-10
    storage.discard_entries_before(11);

    // Leader sends AppendEntries with prev_log_index=10 but WRONG term
    let leader_entries = InMemoryLogEntryCollection::new(&[LogEntry {
        term: 3,
        entry_type: EntryType::Command("cmd16".to_string()),
    }]);

    let (response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        3,  // term
        10, // prev_log_index (at snapshot point)
        3,  // prev_log_term (WRONG - should be 2!)
        leader_entries,
        16,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    match response {
        RaftMsg::AppendEntriesResponse {
            term,
            success,
            match_index: _,
            ..
        } => {
            assert_eq!(term, 3);
            assert!(
                !success,
                "Should reject when prev_log_term doesn't match snapshot term"
            );
        }
        _ => panic!("Expected AppendEntriesResponse"),
    }
}

#[test]
fn test_liveness_both_nodes_compacted_replication_continues() {
    // Test: Both leader and follower have snapshots, normal replication continues
    // Expected: Replication succeeds when both have matching snapshots
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);
    let mut current_term = 2;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Follower has entries 1-20, creates snapshot at 10, keeps 11-20
    let entries: Vec<LogEntry<String>> = (1..=20)
        .map(|i| LogEntry {
            term: 2,
            entry_type: EntryType::Command(format!("cmd{}", i)),
        })
        .collect();
    storage.append_entries(&entries);

    // Create snapshot at index 10, term 2
    for i in 1..=10 {
        state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = state_machine.create_snapshot();

    let snapshot = Snapshot {
        metadata: SnapshotMetadata {
            last_included_index: 10,
            last_included_term: 2,
        },
        data: snapshot_data,
    };
    storage.save_snapshot(snapshot);

    // Compact log - discard entries 1-10
    storage.discard_entries_before(11);

    assert_eq!(storage.first_log_index(), 11);
    assert_eq!(storage.last_log_index(), 20);

    // Leader also compacted to same point, sends new entries 21-22
    // with prev_log_index=20 (entry 20 is in follower's log)
    let leader_entries = InMemoryLogEntryCollection::new(&[
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd21".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd22".to_string()),
        },
    ]);

    let (response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        2,  // term
        20, // prev_log_index (in follower's log)
        2,  // prev_log_term
        leader_entries,
        22,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    match response {
        raft_core::raft_messages::RaftMsg::AppendEntriesResponse {
            term,
            success,
            match_index: _,
            ..
        } => {
            assert_eq!(term, 2);
            assert!(
                success,
                "Should succeed - both nodes compacted, entry 20 exists in log"
            );
        }
        _ => panic!("Expected AppendEntriesResponse"),
    }

    assert_eq!(storage.last_log_index(), 22);
}

#[test]
fn test_safety_follower_rejects_inconsistent_snapshot() {
    // Test: Follower has snapshot with different term than leader expects
    // Expected: Rejects (proves divergence before snapshot point)
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3);
    let mut current_term = 3;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Follower has entries from term 1, then snapshot
    let entries: Vec<LogEntry<String>> = (1..=10)
        .map(|i| LogEntry {
            term: 1, // Different term!
            entry_type: raft_core::log_entry::EntryType::Command(format!("cmd{}", i)),
        })
        .collect();
    storage.append_entries(&entries);

    // Create snapshot at index 10, term 1
    for i in 1..=10 {
        state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = state_machine.create_snapshot();

    let snapshot = Snapshot {
        metadata: SnapshotMetadata {
            last_included_index: 10,
            last_included_term: 1, // Snapshot term is 1
        },
        data: snapshot_data,
    };
    storage.save_snapshot(snapshot);

    storage.discard_entries_before(11);

    // Leader thinks snapshot should be term 2 (different history!)
    let leader_entries = InMemoryLogEntryCollection::new(&[LogEntry {
        term: 3,
        entry_type: EntryType::Command("cmd11".to_string()),
    }]);

    let (response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        3,  // term
        10, // prev_log_index
        2,  // prev_log_term (expects 2, but follower has 1)
        leader_entries,
        11,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    match response {
        RaftMsg::AppendEntriesResponse {
            term,
            success,
            match_index: _,
            ..
        } => {
            assert_eq!(term, 3);
            assert!(
                !success,
                "Should reject - snapshot term mismatch proves divergent history"
            );
        }
        _ => panic!("Expected AppendEntriesResponse"),
    }
}

// ===== InstallSnapshot RPC Tests =====

#[test]
fn test_liveness_leader_detects_follower_needs_snapshot() {
    // Test: Leader's next_index for follower is before first_log_index
    // Expected: Leader should send InstallSnapshot instead of AppendEntries
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);

    // Leader has entries 1-20, creates snapshot at 10, compacts to 11-20
    let entries: Vec<LogEntry<String>> = (1..=20)
        .map(|i| LogEntry {
            term: 2,
            entry_type: EntryType::Command(format!("cmd{}", i)),
        })
        .collect();
    storage.append_entries(&entries);

    let mut state_machine = InMemoryStateMachine::new();
    for i in 1..=10 {
        state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = state_machine.create_snapshot();

    let snapshot = Snapshot {
        metadata: SnapshotMetadata {
            last_included_index: 10,
            last_included_term: 2,
        },
        data: snapshot_data,
    };
    storage.save_snapshot(snapshot);
    storage.discard_entries_before(11);

    // Initialize leader state with 2 peers
    let peers = [0, 1];
    replication.initialize_leader_state(peers.iter().copied(), &storage);

    // Create config for the test
    let mut node_collection = InMemoryNodeCollection::new();
    node_collection.push(0).unwrap();
    node_collection.push(1).unwrap();
    let config = Configuration::new(node_collection);

    // Simulate follower failures to decrement next_index below first_log_index
    // Leader starts with next_index[0] = 21, needs to get it to < 11
    for _ in 0..15 {
        let _config_changes: InMemoryConfigChangeCollection = replication.handle_append_entries_response::<String, InMemoryLogEntryCollection, InMemoryStorage, InMemoryStateMachine, InMemoryNodeCollection, InMemoryConfigChangeCollection>(
            0,     // peer
            false, // failure
            0,     // match_index (ignored on failure)
            &storage,
            &mut InMemoryStateMachine::new(),
            &config,
        );
    }

    // Get message for follower - should be InstallSnapshot
    let message = replication.get_append_entries_for_peer(0, 0, &storage);

    match message {
        RaftMsg::InstallSnapshot {
            term,
            leader_id: _,
            last_included_index,
            last_included_term,
            offset,
            data,
            done,
        } => {
            assert_eq!(term, 2);
            assert_eq!(last_included_index, 10);
            assert_eq!(last_included_term, 2);
            assert_eq!(offset, 0);
            assert!(!data.to_vec().is_empty(), "Should include snapshot data");
            assert!(done, "Should mark as done for single chunk");
        }
        _ => panic!("Expected InstallSnapshot, got {:?}", message),
    }
}

#[test]
fn test_liveness_follower_installs_snapshot_successfully() {
    // Test: Follower receives InstallSnapshot and installs it correctly
    // Expected: Snapshot installed, old log discarded, state machine updated
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);
    let mut current_term = 2;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Follower has old entries 1-5, term 1 (stale)
    let old_entries: Vec<LogEntry<String>> = (1..=5)
        .map(|i| LogEntry {
            term: 1,
            entry_type: EntryType::Command(format!("old_cmd{}", i)),
        })
        .collect();
    storage.append_entries(&old_entries);

    // Leader sends snapshot covering entries 1-10
    let mut leader_state_machine = InMemoryStateMachine::new();
    for i in 1..=10 {
        leader_state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = leader_state_machine.create_snapshot();
    let snapshot_bytes = snapshot_data.to_bytes().to_vec();

    let response = replication.handle_install_snapshot(
        2,  // term
        0,  // leader_id
        10, // last_included_index
        2,  // last_included_term
        0,  // offset
        InMemoryChunkCollection::from_vec(snapshot_bytes),
        true, // done
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role,
    );

    match response {
        RaftMsg::InstallSnapshotResponse { term, success } => {
            assert_eq!(term, 2);
            assert!(success, "Should succeed installing snapshot");
        }
        _ => panic!("Expected InstallSnapshotResponse"),
    }

    // Verify snapshot was installed
    let metadata = storage.snapshot_metadata().expect("Should have snapshot");
    assert_eq!(metadata.last_included_index, 10);
    assert_eq!(metadata.last_included_term, 2);

    // Verify old log was discarded
    assert_eq!(storage.first_log_index(), 11);

    // Verify state machine was restored
    assert_eq!(state_machine.get("cmd10"), Some("value10"));
}

#[test]
fn test_safety_follower_rejects_stale_snapshot() {
    // Test: Follower already has more recent snapshot than leader is sending
    // Expected: Rejects snapshot (already applied)
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3);
    let mut current_term = 3;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Follower already has snapshot at index 15
    for i in 1..=15 {
        state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let current_snapshot_data = state_machine.create_snapshot();

    let current_snapshot = Snapshot {
        metadata: SnapshotMetadata {
            last_included_index: 15,
            last_included_term: 3,
        },
        data: current_snapshot_data,
    };
    storage.save_snapshot(current_snapshot);
    storage.discard_entries_before(16);

    // Leader tries to send older snapshot (index 10)
    let mut old_state_machine = InMemoryStateMachine::new();
    for i in 1..=10 {
        old_state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let old_snapshot_data = old_state_machine.create_snapshot();
    let snapshot_bytes = old_snapshot_data.to_bytes().to_vec();

    let response = replication.handle_install_snapshot(
        3,  // term
        0,  // leader_id
        10, // last_included_index (older than follower's 15)
        2,  // last_included_term
        0,  // offset
        InMemoryChunkCollection::from_vec(snapshot_bytes),
        true, // done
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role,
    );

    match response {
        RaftMsg::InstallSnapshotResponse { term, success } => {
            assert_eq!(term, 3);
            assert!(!success, "Should reject stale snapshot");
        }
        _ => panic!("Expected InstallSnapshotResponse"),
    }

    // Verify original snapshot unchanged
    let metadata = storage.snapshot_metadata().expect("Should have snapshot");
    assert_eq!(metadata.last_included_index, 15);
}

#[test]
fn test_liveness_snapshot_transfer_with_chunks() {
    // Test: Large snapshot transferred in multiple chunks
    // Expected: Chunks assembled correctly, snapshot installed
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);
    let mut current_term = 2;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Create snapshot data
    let mut leader_state_machine = InMemoryStateMachine::new();
    for i in 1..=10 {
        leader_state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = leader_state_machine.create_snapshot();
    let snapshot_bytes = snapshot_data.to_bytes().to_vec();

    // Split into 3 chunks
    let chunk_size = snapshot_bytes.len() / 3;
    let chunk1 = &snapshot_bytes[0..chunk_size];
    let chunk2 = &snapshot_bytes[chunk_size..chunk_size * 2];
    let chunk3 = &snapshot_bytes[chunk_size * 2..];

    // Send chunk 1
    let response1 = replication.handle_install_snapshot(
        2,
        0,
        10,
        2,
        0,
        InMemoryChunkCollection::from_vec(chunk1.to_vec()),
        false, // not done
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role,
    );

    match response1 {
        RaftMsg::InstallSnapshotResponse { term, success } => {
            assert_eq!(term, 2);
            assert!(success, "Should accept chunk 1");
        }
        _ => panic!("Expected InstallSnapshotResponse"),
    }

    // Send chunk 2
    let response2 = replication.handle_install_snapshot(
        2,
        0,
        10,
        2,
        chunk_size as u64,
        InMemoryChunkCollection::from_vec(chunk2.to_vec()),
        false, // not done
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role,
    );

    match response2 {
        RaftMsg::InstallSnapshotResponse { term, success } => {
            assert_eq!(term, 2);
            assert!(success, "Should accept chunk 2");
        }
        _ => panic!("Expected InstallSnapshotResponse"),
    }

    // Send final chunk
    let response3 = replication.handle_install_snapshot(
        2,
        0,
        10,
        2,
        (chunk_size * 2) as u64,
        InMemoryChunkCollection::from_vec(chunk3.to_vec()),
        true, // done
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role,
    );

    match response3 {
        RaftMsg::InstallSnapshotResponse { term, success } => {
            assert_eq!(term, 2);
            assert!(success, "Should complete snapshot installation");
        }
        _ => panic!("Expected InstallSnapshotResponse"),
    }

    // Verify snapshot was installed
    let metadata = storage.snapshot_metadata().expect("Should have snapshot");
    assert_eq!(metadata.last_included_index, 10);
    assert_eq!(metadata.last_included_term, 2);

    // Verify state machine was restored
    assert_eq!(state_machine.get("cmd10"), Some("value10"));
}

#[test]
fn test_liveness_replication_resumes_after_snapshot_install() {
    // Test: After installing snapshot, normal AppendEntries replication resumes
    // Expected: Follower accepts new entries after snapshot
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);
    let mut current_term = 2;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Install snapshot covering entries 1-10
    let mut leader_state_machine = InMemoryStateMachine::new();
    for i in 1..=10 {
        leader_state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = leader_state_machine.create_snapshot();
    let snapshot_bytes = snapshot_data.to_bytes().to_vec();

    replication.handle_install_snapshot(
        2,
        0,
        10,
        2,
        0,
        InMemoryChunkCollection::from_vec(snapshot_bytes),
        true,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role,
    );

    // Now leader sends normal AppendEntries with new entries 11-12
    let new_entries = InMemoryLogEntryCollection::new(&[
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd11".to_string()),
        },
        LogEntry {
            term: 2,
            entry_type: EntryType::Command("cmd12".to_string()),
        },
    ]);

    let (response, _config_changes) = replication.handle_append_entries::<String, InMemoryLogEntryCollection, InMemoryChunkCollection, InMemoryConfigChangeCollection, InMemoryStorage, InMemoryStateMachine>(
        2,  // term
        10, // prev_log_index (at snapshot point)
        2,  // prev_log_term
        new_entries,
        12,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role
    );

    match response {
        RaftMsg::AppendEntriesResponse {
            term,
            success,
            match_index,
            ..
        } => {
            assert_eq!(term, 2);
            assert!(success, "Should accept entries after snapshot");
            assert_eq!(match_index, 12);
        }
        _ => panic!("Expected AppendEntriesResponse"),
    }

    // Verify entries were appended
    assert_eq!(storage.last_log_index(), 12);
}

#[test]
fn test_safety_follower_rejects_snapshot_from_old_term() {
    // Test: Follower rejects snapshot from leader with older term
    // Expected: Rejects and updates term
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(3); // Follower is on term 3
    let mut current_term = 3;
    let mut role = NodeState::Follower;
    let mut state_machine = InMemoryStateMachine::new();

    // Leader sends snapshot from old term 2
    let mut old_state_machine = InMemoryStateMachine::new();
    for i in 1..=10 {
        old_state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = old_state_machine.create_snapshot();
    let snapshot_bytes = snapshot_data.to_bytes().to_vec();

    let response = replication.handle_install_snapshot(
        2, // old term
        0,
        10,
        2,
        0,
        InMemoryChunkCollection::from_vec(snapshot_bytes),
        true,
        &mut current_term,
        &mut storage,
        &mut state_machine,
        &mut role,
    );

    match response {
        RaftMsg::InstallSnapshotResponse { term, success } => {
            assert_eq!(term, 3, "Should respond with current term");
            assert!(!success, "Should reject snapshot from old term");
        }
        _ => panic!("Expected InstallSnapshotResponse"),
    }

    // Verify no snapshot was installed
    assert!(storage.snapshot_metadata().is_none());
}

#[test]
fn test_liveness_leader_updates_next_index_after_snapshot_success() {
    // Test: Leader updates next_index after successful InstallSnapshot
    // Expected: next_index set to last_included_index + 1
    let mut replication = LogReplicationManager::<InMemoryMapCollection>::new();
    let mut storage = InMemoryStorage::new();
    storage.set_current_term(2);

    // Leader has snapshot at 10
    let entries: Vec<LogEntry<String>> = (1..=20)
        .map(|i| LogEntry {
            term: 2,
            entry_type: EntryType::Command(format!("cmd{}", i)),
        })
        .collect();
    storage.append_entries(&entries);

    let mut state_machine = InMemoryStateMachine::new();
    for i in 1..=10 {
        state_machine.apply(&format!("SET cmd{}=value{}", i, i));
    }
    let snapshot_data = state_machine.create_snapshot();

    let snapshot = Snapshot {
        metadata: SnapshotMetadata {
            last_included_index: 10,
            last_included_term: 2,
        },
        data: snapshot_data,
    };
    storage.save_snapshot(snapshot);
    storage.discard_entries_before(11);

    // Initialize leader state
    let peers = [0, 1];
    replication.initialize_leader_state(peers.iter().copied(), &storage);

    // Handle successful InstallSnapshotResponse
    replication.handle_install_snapshot_response(
        0,    // peer_id
        2,    // term
        true, // success
        10,   // last_included_index from snapshot
    );

    // Verify next_index updated to snapshot point + 1
    let next_msg = replication.get_append_entries_for_peer(0, 0, &storage);
    match next_msg {
        RaftMsg::AppendEntries { prev_log_index, .. } => {
            assert_eq!(
                prev_log_index, 10,
                "next_index should be updated to snapshot point + 1"
            );
        }
        _ => panic!("Should send AppendEntries after snapshot success"),
    }
}
